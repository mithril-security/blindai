# Copyright 2022 Mithril Security. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import contextlib
from functools import wraps
import getpass
import logging
import os
import pkgutil
import socket
import ssl
import platform
from hashlib import sha256
from typing import Any, List, Optional, Tuple, Union

from cryptography.exceptions import InvalidSignature
from grpc import Channel, RpcError, secure_channel, ssl_channel_credentials
from cryptography.hazmat.primitives.asymmetric.ed25519 import Ed25519PublicKey

from blindai.dcap_attestation import (
    Policy,
    verify_claims,
    verify_dcap_attestation,
)

# These modules are generated by grpc proto compiler, from proto files in proto
import blindai.pb as _
from blindai.pb.securedexchange_pb2 import (
    Payload as PbPayload,
    RunModelRequest as PbRunModelRequest,
    RunModelReply as PbRunModelReply,
    SendModelRequest as PbSendModelRequest,
    ClientInfo,
    TensorInfo as PbTensorInfo,
    DeleteModelRequest as PbDeleteModelRequest,
    TensorData as PbTensorData,
)
import grpc
import blindai.pb.licensing_pb2 as licensing_pb2
import blindai.pb.licensing_pb2_grpc as licensing_pb2_grpc
from blindai.pb.proof_files_pb2 import ResponseProof
from blindai.pb.securedexchange_pb2_grpc import ExchangeStub
from blindai.pb.untrusted_pb2 import GetCertificateRequest as certificate_request
from blindai.pb.untrusted_pb2 import GetServerInfoRequest as server_info_request
from blindai.pb.untrusted_pb2 import GetSgxQuoteWithCollateralReply
from blindai.pb.untrusted_pb2 import GetSgxQuoteWithCollateralRequest as quote_request
from blindai.pb.untrusted_pb2_grpc import AttestationStub
from blindai.utils.errors import (
    SignatureError,
    VersionError,
    HardwareModeUnsupportedError,
    check_rpc_exception,
    check_socket_exception,
)
from blindai.utils.utils import (
    create_byte_chunk,
    encode_certificate,
    get_enclave_signing_key,
    strip_https,
    supported_server_version,
    get_supported_server_version,
    ModelDatumType,
)
from blindai.version import __version__ as app_version

from blindai.utils.serialize import deserialize_tensor, serialize_tensor

MITHRIL_SERVICES_URL = os.getenv("MITHRIL_SERVICES_URL", "api.cloud.mithrilsecurity.io")
MITHRIL_SERVICES_INSECURE = os.getenv("MITHRIL_SERVICES_INSECURE") == "true"

if not MITHRIL_SERVICES_INSECURE:
    if "MITHRIL_SERVICES_POLICY" in os.environ:
        with os.open(os.getenv("MITHRIL_SERVICES_POLICY")) as f:
            MITHRIL_SERVICES_POLICY = f.read()
    else:
        MITHRIL_SERVICES_POLICY = pkgutil.get_data(
            __name__, "tls/mithril_services_policy.toml"
        ).decode("utf-8")
else:
    MITHRIL_SERVICES_POLICY = False

CONNECTION_TIMEOUT = 10

log = logging.getLogger(__name__)


def dtype_to_numpy(dtype: ModelDatumType) -> str:
    translation_map = {
        ModelDatumType.F32: "float32",
        ModelDatumType.F64: "float64",
        ModelDatumType.I32: "int32",
        ModelDatumType.I64: "int64",
        ModelDatumType.U32: "uint32",
        ModelDatumType.U64: "uint64",
        ModelDatumType.U8: "uint8",
        ModelDatumType.U16: "uint16",
        ModelDatumType.I8: "int8",
        ModelDatumType.I16: "int16",
        ModelDatumType.Bool: "bool",
    }
    if dtype not in translation_map:
        raise ValueError(f"Numpy does not support datum type {dtype}.")
    return translation_map[dtype]


def dtype_to_torch(dtype: ModelDatumType) -> str:
    translation_map = {
        ModelDatumType.F32: "float32",
        ModelDatumType.F64: "float64",
        ModelDatumType.I32: "int32",
        ModelDatumType.I64: "int64",
        # ModelDatumType.U32: "uint32",
        # ModelDatumType.U64: "uint64",
        ModelDatumType.U8: "uint8",
        # ModelDatumType.U16: "uint16",
        ModelDatumType.I8: "int8",
        ModelDatumType.I16: "int16",
        ModelDatumType.Bool: "bool",
    }
    if dtype not in translation_map:
        raise ValueError(f"Torch does not support datum type {dtype}.")
    return translation_map[dtype]


def translate_dtype(dtype):
    if isinstance(dtype, ModelDatumType):
        return dtype

    elif type(dtype).__module__ == "numpy" and type(dtype).__name__.startswith("dtype"):
        numpy_dtype_translation = {
            "float32": ModelDatumType.F32,
            "float64": ModelDatumType.F64,
            "int32": ModelDatumType.I32,
            "int64": ModelDatumType.I64,
            "uint32": ModelDatumType.U32,
            "uint64": ModelDatumType.U64,
            "uint8": ModelDatumType.U8,
            "uint16": ModelDatumType.U16,
            "int8": ModelDatumType.I8,
            "int16": ModelDatumType.I16,
            "bool": ModelDatumType.Bool,
        }
        if str(dtype) not in numpy_dtype_translation:
            raise ValueError(f"Numpy dtype {str(dtype)} is not supported.")
        return numpy_dtype_translation[str(dtype)]

    if type(dtype).__module__ == "torch" and type(dtype).__name__ == "dtype":
        # Torch does not support unsigned ints except u8.
        torch_dtype_translation = {
            "torch.float32": ModelDatumType.F32,
            "torch.float64": ModelDatumType.F64,
            "torch.int32": ModelDatumType.I32,
            "torch.int64": ModelDatumType.I64,
            # "torch.uint32": ModelDatumType.U32,
            # "torch.uint64": ModelDatumType.U64,
            "torch.uint8": ModelDatumType.U8,
            # "torch.uint16": ModelDatumType.U16,
            "torch.int8": ModelDatumType.I8,
            "torch.int16": ModelDatumType.I16,
            "torch.bool": ModelDatumType.Bool,
        }
        if str(dtype) not in torch_dtype_translation:
            raise ValueError(f"Torch dtype {str(dtype)} is not supported.")
        return torch_dtype_translation[str(dtype)]

    if isinstance(dtype, str):
        str_dtype_translation = {
            "float32": ModelDatumType.F32,
            "f32": ModelDatumType.F32,
            "float64": ModelDatumType.F64,
            "f64": ModelDatumType.F64,
            "int32": ModelDatumType.I32,
            "i32": ModelDatumType.I32,
            "int64": ModelDatumType.I64,
            "i64": ModelDatumType.I64,
            "uint32": ModelDatumType.U32,
            "u32": ModelDatumType.U32,
            "uint64": ModelDatumType.U64,
            "u64": ModelDatumType.U64,
            "uint8": ModelDatumType.U8,
            "u8": ModelDatumType.U8,
            "uint16": ModelDatumType.U16,
            "u16": ModelDatumType.U16,
            "int8": ModelDatumType.I8,
            "i8": ModelDatumType.I8,
            "int16": ModelDatumType.I16,
            "i16": ModelDatumType.I16,
            "bool": ModelDatumType.Bool,
        }
        if dtype.lower() not in str_dtype_translation:
            raise ValueError(f"Datum type {dtype} is not understood.")
        return str_dtype_translation[dtype.lower()]

    raise ValueError(
        f"DatumType instance {type(dtype).__module__}.{type(dtype).__name__} not supported"
    )


def translate_tensors(tensors, dtypes, shapes):
    ret = []

    # list of {numpy/torch/flat} tensors are valid inputs, and are treated as multiple inputs
    # direct numpy/torch/flat tensors are valid inputs, and are treated as a single input, which
    #  will be wrapped into a 1-el list on the folowing statement
    #
    # flat list means list[int], and is the flattened tensor
    #  this means that you must specify dtype/shape for this tensor! on the other cases, it's redundant
    # (todo: accept iterables instead of flat list only)
    #
    # mental note
    # - anything not list should be wrapped into [X]
    # - list[int] should be wrapped into [X]
    # - but! list[list[int]] is should be unchanged
    if not isinstance(tensors, list) or (
        len(tensors) > 0 and isinstance(tensors[0], list)
    ):
        tensors = [tensors]

    # same logic applies for dtypes/shapes
    if dtypes is not None and not isinstance(dtypes, list):
        dtypes = [dtypes]
    if not isinstance(shapes, list) or (
        len(shapes) > 0
        and not (isinstance(shapes[0], list) or isinstance(shapes[0], tuple))
    ):
        shapes = [shapes]

    for i, tensor in enumerate(tensors):
        or_dtype = dtypes[i] if dtypes is not None and len(dtypes) > i else None
        or_shape = shapes[i] if shapes is not None and len(shapes) > i else None

        if type(tensor).__module__ == "torch" and type(tensor).__name__ == "Tensor":
            dtype = translate_dtype(tensor.dtype)
            shape = list(tensor.shape)
            iterable = tensor.flatten()

        elif type(tensor).__module__ == "numpy" and type(tensor).__name__ == "ndarray":
            dtype = translate_dtype(tensor.dtype)
            shape = list(tensor.shape)
            iterable = tensor.flatten()

        else:
            # Input is flatten tensor.
            if not isinstance(tensor, list):
                raise ValueError(
                    f"Input tensor has an unsupported type: {type(tensor).__module__}.{type(tensor).__name__}"
                )

            dtype = translate_dtype(or_dtype)
            shape = list(or_shape)
            iterable = tensor

        if or_dtype is not None and or_dtype != dtype:
            raise ValueError(
                f"Given tensor has dtype {str(tensor.dtype)}, but {or_dtype} was expected."
            )
        if or_shape is not None and list(or_shape) != list(shape):
            raise ValueError(
                f"Given tensor has shape {list(tensor.shape)}, but {or_shape} was expected."
            )

        # todo validate tensor content, dtype and shape

        ret.append((iterable, dtype, shape))

    return ret


def _validate_quote(
    attestation: GetSgxQuoteWithCollateralReply, policy: Policy
) -> Ed25519PublicKey:
    """Returns the enclave signing key"""

    claims = verify_dcap_attestation(
        attestation.quote, attestation.collateral, attestation.enclave_held_data
    )

    verify_claims(claims, policy)
    server_cert = claims.get_server_cert()
    enclave_signing_key = get_enclave_signing_key(server_cert)

    return enclave_signing_key


def _get_input_output_tensors(
    input_specs: Optional[List[List[Any]]],
    output_specs: Optional[ModelDatumType],
    shape: Tuple,
    datum_type: ModelDatumType,
    dtype_out: ModelDatumType,
) -> Tuple[List[List[Any]], List[ModelDatumType]]:
    if input_specs is None and (datum_type is None or shape is None):
        input_specs = []
    if output_specs is None and dtype_out is None:
        output_specs = []

    if input_specs is None:
        input_specs = [shape, datum_type]
    if output_specs is None:
        output_specs = [dtype_out]

    if len(input_specs) > 0 and type(input_specs[0]) != list:
        input_specs = [input_specs]
    if len(output_specs) > 0 and type(output_specs[0]) != list:
        output_specs = [output_specs]

    inputs = []
    for i, tensor_input in enumerate(input_specs):
        dtype = (
            translate_dtype(tensor_input[1]) if tensor_input[1] is not None else None
        )
        inputs.append(
            PbTensorInfo(
                dims=tensor_input[0],
                datum_type=translate_dtype(tensor_input[1]),
                index=i,
            )
        )

    outputs = []
    for i, tensor_output in enumerate(output_specs):
        dtype = (
            translate_dtype(tensor_output[0]) if tensor_output[0] is not None else None
        )
        outputs.append(PbTensorInfo(datum_type=dtype, index=i))

    return (inputs, outputs)


class SignedResponse:
    payload: Optional[bytes] = None
    """Raw protobuf object of the response from the server. Used to verify if the request was not altered by a third party."""
    signature: Optional[bytes] = None
    """Signature of the payload made by the server. Allows to verify if the object was not changed by a third party."""
    attestation: Optional[GetSgxQuoteWithCollateralReply] = None
    """Contains the attestation provided by the enclave, if connected to a server in hardware mode."""

    def is_simulation_mode(self) -> bool:
        return self.attestation is None

    def is_signed(self) -> bool:
        return self.signature is not None

    def save_to_file(self, path: str):
        """Save the response to a file.
        The response can later be loaded with:

        ```py
        res = SignedResponse()
        res.load_from_file(path)
        ```

        Args:
            path (str): Path of the file.
        """
        with open(path, mode="wb") as file:
            file.write(self.as_bytes())

    def as_bytes(self) -> bytes:
        """Save the response as bytes.
        The response can later be loaded with:

        ```py
        res = SignedResponse()
        res.load_from_bytes(data)
        ```

        Returns:
            bytes: The data.
        """
        return ResponseProof(
            payload=self.payload,
            signature=self.signature,
            attestation=self.attestation,
        ).SerializeToString()

    def load_from_file(self, path: str):
        """Load the response from a file.

        Args:
            path (str): Path of the file.
        """
        with open(path, "rb") as file:
            self.load_from_bytes(file.read())

    def load_from_bytes(self, b: bytes):
        """Load the response from bytes.

        Args:
            b (bytes): The data.
        """
        proof = ResponseProof.FromString(b)
        self.payload = proof.payload
        self.signature = proof.signature
        self.attestation = proof.attestation
        self._load_payload()

    def _load_payload(self):
        pass


class UploadModelResponse(SignedResponse):
    model_id: str

    def validate(
        self,
        model_hash: bytes,
        policy_file: Optional[str] = None,
        policy: Optional[Policy] = None,
        validate_quote: bool = True,
        enclave_signing_key: Optional[bytes] = None,
        allow_simulation_mode: bool = False,
    ):
        """Validates whether this response is valid. This is used for responses you have saved as bytes or in a file.
        This will raise an error if the response is not signed or if it is not valid.

        ***Security & confidentiality warnings:***<br>
        *`validate_quote` and `enclave_signing_key` : in case where the quote validation is set, the `enclave_signing_key` is generated directly using the certificate and the policy file and assigned otherwise.
        The hash of the `enclave_signing_key` is then represented as the MRSIGNER hash.*

        Args:
            model_hash (bytes): Hash of the model to verify against.
            policy_file (Optional[str], optional): Path to the policy file. Defaults to None.
            policy (Optional[Policy], optional): Policy to use. Use `policy_file` to load from a file directly. Defaults to None.
            validate_quote (bool, optional): Whether or not the attestation should be validated too. Defaults to True.
            enclave_signing_key (Optional[bytes], optional): Enclave signing key in case the attestation should not be validated. Defaults to None.
            allow_simulation_mode (bool, optional): Whether or not simulation mode responses should be accepted. Defaults to False.

        Raises:
            AttestationError: Attestation is invalid.
            SignatureError: Signed response is invalid.
            FileNotFoundError: Will be raised if the policy file is not found.
        """
        if not self.is_signed():
            raise SignatureError("Response is not signed")

        if not allow_simulation_mode and self.is_simulation_mode():
            raise SignatureError("Response was produced using simulation mode")

        if not self.is_simulation_mode() and validate_quote and policy_file is not None:
            policy = Policy.from_file(policy_file)

        # Quote validation

        if not self.is_simulation_mode() and validate_quote:
            enclave_signing_key = _validate_quote(self.attestation, policy)

        # Payload validation

        payload = PbPayload.FromString(self.payload).send_model_payload
        if not self.is_simulation_mode():
            try:
                enclave_signing_key.verify(self.signature, self.payload)
            except InvalidSignature:
                raise SignatureError("Invalid signature")

        # Input validation

        if model_hash != payload.model_hash:
            raise SignatureError("Invalid response model_hash")

    def _load_payload(self):
        payload = PbPayload.FromString(self.payload).send_model_payload
        self.model_id = payload.model_id


class TensorInfo:
    dims: List[int]
    datum_type: ModelDatumType
    index: int
    index_name: str

    def __init__(
        self, dims: List[int], datum_type: ModelDatumType, index: int, index_name: str
    ):
        self.dims = dims
        self.datum_type = datum_type


class Tensor:
    info: TensorInfo
    bytes_data: bytes

    def __init__(self, info: TensorInfo, bytes_data: bytes):
        self.info = info
        self.bytes_data = bytes_data

    def as_flat(self) -> list:
        """Convert the prediction calculated by the server to a flat python list."""
        return list(deserialize_tensor(self.bytes_data, self.info.datum_type))

    def as_numpy(self):
        """Convert the prediction calculated by the server to a numpy array."""
        import numpy

        arr = numpy.array([*self.as_flat()], dtype=dtype_to_numpy(self.info.datum_type))
        arr.shape = self.shape
        return arr

    def as_torch(self):
        """Convert the prediction calculated by the server to a Torch Tensor."""
        import torch

        arr = torch.asarray(
            [*self.as_flat()],
            dtype=getattr(torch, dtype_to_torch(self.info.datum_type)),
        )
        arr.view(self.shape)
        return arr

    @property
    def shape(self) -> tuple:
        return tuple(self.info.dims)

    @property
    def datum_type(self) -> ModelDatumType:
        return self.info.datum_type


class PredictResponse(SignedResponse):
    """Contains the inference calculated by the server, alongside the data needed to verify the integrity of the data sent."""

    output: List[Tensor] = None
    """Contains the inference calculated by the server. Act as an array. To extract the first prediction, please use [0]. Can be converted to a Torch Tensor, a numpy array of a flat list."""
    model_id: str = None
    """Model ID of the model, on the server."""
    inference_time: int = 0
    """Time spent to do the inference on the server. Will be set to 0 if the server does not share this data."""
    request_time: int = 0
    """Time spent to handles the whole request. Will be set to 0 if the server does not share this data."""

    def __init__(
        self,
        input_tensors: Union[List[List[Any]], List[Any]] = None,
        input_datum_type: Union[List[ModelDatumType], ModelDatumType] = None,
        input_shape: Union[List[List[int]], List[int]] = None,
        response: PbRunModelReply = None,
        sign: bool = True,
        attestation: Optional[GetSgxQuoteWithCollateralReply] = None,
        enclave_signing_key: Optional[bytes] = None,
        allow_simulation_mode: bool = False,
    ):
        if response is None:
            return

        payload = PbPayload.FromString(response.payload).run_model_payload
        self.output = [
            Tensor(
                info=TensorInfo(
                    tensor.info.dims,
                    tensor.info.datum_type,
                    tensor.info.index,
                    tensor.info.index_name,
                ),
                bytes_data=tensor.bytes_data,
            )
            for tensor in payload.output_tensors
        ]
        self.model_id = payload.model_id
        self.inference_time = payload.inference_time
        self.request_time = payload.request_time

        # Response Verification
        if sign:
            self.payload = response.payload
            self.signature = response.signature
            self.attestation = attestation
            log.debug("Veryfing integrity of the data sent...")
            self.validate(
                self.model_id,
                input_tensors,
                input_datum_type,
                input_shape,
                validate_quote=False,
                enclave_signing_key=enclave_signing_key,
                allow_simulation_mode=allow_simulation_mode,
            )
            log.info("Integrity of the data uploaded verified.")
        else:
            log.info("Response validity and integrity NOT verified.")

    def validate(
        self,
        model_id: str,
        tensors: Union[List[List[Any]], List[Any]],
        dtype: Union[List[ModelDatumType], ModelDatumType] = None,
        shape: Union[List[List[int]], List[int]] = None,
        policy_file: Optional[str] = None,
        policy: Optional[Policy] = None,
        validate_quote: bool = True,
        enclave_signing_key: Optional[bytes] = None,
        allow_simulation_mode: bool = False,
    ):
        """Validates whether this response is valid. This is used for responses you have saved as bytes or in a file.
        This will raise an error if the response is not signed or if it is not valid.

        ***Security & confidentiality warnings:***<br>
            *`validate_quote` and `enclave_signing_key` : in case where the quote validation is set, the `enclave_signing_key` is generated directly using the certificate and the policy file and assigned otherwise.
            The hash of the `enclave_signing_key` is then represented as the MRSIGNER hash.
            When the simulation mode is off, the attestation is verified, and only in that case, the data is processed while assigning `data_list`*

        Args:
            model_id (str): The model id to check against.
            tensors (List[Any]): Input used to run the model, to validate against.
            policy_file (Optional[str], optional): Path to the policy file. Defaults to None.
            policy (Optional[Policy], optional): Policy to use. Use `policy_file` to load from a file directly. Defaults to None.
            validate_quote (bool, optional): Whether or not the attestation should be validated too. Defaults to True.
            enclave_signing_key (Optional[bytes], optional): Enclave signing key in case the attestation should not be validated. Defaults to None.
            allow_simulation_mode (bool, optional): Whether or not simulation mode responses should be accepted. Defaults to False.

        Raises:
            AttestationError: Attestation is invalid.
            SignatureError: Signed response is invalid.
            FileNotFoundError: Will be raised if the policy file is not found.
        """
        if not self.is_signed():
            raise SignatureError("Response is not signed")

        if not allow_simulation_mode and self.is_simulation_mode():
            raise SignatureError("Response was produced using simulation mode")

        if not self.is_simulation_mode() and validate_quote and policy_file is not None:
            policy = Policy.from_file(policy_file)

        # Quote validation

        if not self.is_simulation_mode() and validate_quote:
            enclave_signing_key = _validate_quote(self.attestation, policy)

        # Payload validation

        payload = PbPayload.FromString(self.payload).run_model_payload
        if not self.is_simulation_mode():
            try:
                enclave_signing_key.verify(self.signature, self.payload)
            except InvalidSignature:
                raise SignatureError("Invalid signature")

        hash = sha256()
        for tensor_iterable, tensor_dtype, _tensor_shape in translate_tensors(
            tensors, dtype, shape
        ):
            for chunk in serialize_tensor(tensor_iterable, tensor_dtype):
                hash.update(chunk)

        if hash.digest() != payload.input_hash:
            raise SignatureError("Invalid response input_hash")
        log.info(f"Hash of data uploaded: {hash.hexdigest()}")

        if model_id != payload.model_id:
            raise SignatureError("Invalid response model_id")

    def _load_payload(self):
        payload = PbPayload.FromString(self.payload).run_model_payload
        self.output = [
            Tensor(
                info=TensorInfo(
                    tensor.info.dims,
                    tensor.info.datum_type,
                    tensor.info.index,
                    tensor.info.index_name,
                ),
                bytes_data=tensor.bytes_data,
            )
            for tensor in payload.output_tensors
        ]
        self.model_id = payload.model_id
        self.inference_time = payload.inference_time
        self.request_time = payload.request_time


class DeleteModelResponse:
    pass


def raise_exception_if_conn_closed(f):
    """
    Decorator which raises an exception if the Connection is closed before calling
    the decorated method
    """

    @wraps(f)
    def wrapper(self, *args, **kwds):
        if self.closed:
            raise ValueError("Illegal operation on closed connection.")
        return f(self, *args, **kwds)

    return wrapper


class Connection(contextlib.AbstractContextManager):
    _channel: Optional[Channel] = None
    policy: Optional[Policy] = None
    _stub: Optional[ExchangeStub] = None
    enclave_signing_key: Optional[bytes] = None
    simulation_mode: bool = False
    attestation: Optional[GetSgxQuoteWithCollateralReply] = None
    server_version: Optional[str] = None
    client_info: ClientInfo
    input_specs: Optional[List[List[Any]]]
    output_specs: Optional[List[ModelDatumType]]
    closed: bool = False
    _jwt: Optional[str] = None

    def _grpc_call_metadata(self):
        if self._jwt is not None:
            return (("accesstoken", self._jwt),)
        else:
            return ()

    def __init__(
        self,
        addr: Optional[str] = None,
        server_name: str = "blindai-srv",
        policy: Optional[str] = None,
        certificate: Optional[str] = None,
        simulation: bool = False,
        untrusted_port: int = 50052,
        attested_port: int = 50051,
        debug_mode=False,
        api_key: Optional[str] = None,
    ):
        """Connect to the server with the specified parameters.
        You will have to specify here the expected policy (server identity, configuration...)
        and the server TLS certificate, if you are using the hardware mode.

        If you want to use Mithril Security Cloud, you don't need to specify the address, the policy and the certificate. Those informations will be automatically retreived.

        If you're using the simulation mode, you don't need to provide a policy and certificate,
        but please keep in mind that this mode should NEVER be used in production as it doesn't
        have most of the security provided by the hardware mode.

        ***Security & confidentiality warnings:***
           *policy: Defines the rules upon which enclaves are accepted (after quote data verification). Contains the hash of MRENCLAVE which helps identify code and data of an enclave. In the case of leakeage of this file, data & model confidentiality would not be affected as the information just serves as a verification check.
           For more details, the attestation info is verified against the policy for the quote. In case of a leakage of the information of this file, code and data inside the secure enclave will remain inaccessible.
           certificate:  The certificate file, which is also generated server side, is used to assigned the claims the policy is checked against. It serves to identify the server for creating a secure channel and begin the attestation process.*

        Args:
            addr (str): The address of BlindAI server you want to reach. If you don't specify anything, you will be automatically connected to Mithril Security Cloud.
            server_name (str, optional): Contains the CN expected by the server TLS certificate. Defaults to "blindai-srv".
            policy (Optional[str], optional): Path to the toml file describing the policy of the server.
                Generated in the server side. Defaults to None. Will be ignored if you are in simulation mode or trying to connect to the Mithril Security Cloud.
                If left to none and if you are in hardware mode, the built-in policy (policy matching to the 0.5 version of the server, or the Mithril Security Cloud enclave) will be used.
            certificate (Optional[str], optional): Path to the public key of the untrusted inference server.
                Generated in the server side. Defaults to None. Will be ignored if you are in simulation mode or trying to connect to the Mithril Security Cloud.
                If left to none and if you are in hardware mode, the certificate verification will be disabled
            simulation (bool, optional): Connect to the server in simulation mode.
                If set to True, the args policy and certificate will be ignored. Defaults to False.
            untrusted_port (int, optional): Untrusted connection server port. Defaults to 50052.
            attested_port (int, optional): Attested connection server port. Defaults to 50051.
            debug_mode (bool, optional): Prints debug message, will also turn on GRPC log messages.
            api_key (str, optional): Key to upload and use your models on Mithril Security Cloud. This parameter is not needed if you want to use the public models, or if you want to deploy the server yourself.

        Raises:
            AttestationError: Will be raised if the policy doesn't match the server configuration, or if the attestation is invalid.
            NotAnEnclaveError: Will be raised if the enclave claims are not validated by the hardware provider, meaning that the claims cannot be verified using the hardware root of trust.
            IdentityError: Will be raised if the enclave code signature hash does not match the signature hash provided in the policy.
            DebugNotAllowedError: Will be raised if the enclave is in debug mode but the provided policy doesn't allow debug mode.
            HardwareModeUnsupportedError: will be raised if the server is in simulation mode but an hardware mode attestation was requested from it.
            ConnectionError: will be raised if the connection with the server fails.
            VersionError: Will be raised if the version of the server is not supported by the client.
            FileNotFoundError: will be raised if the policy file, or the certificate file is not
                found (in Hardware mode).
        """
        if debug_mode:  # pragma: no cover
            os.environ["GRPC_TRACE"] = "transport_security,tsi"
            os.environ["GRPC_VERBOSITY"] = "DEBUG"

        use_cloud = addr is None

        if use_cloud is True:
            # Use Mithril Cloud services.
            if MITHRIL_SERVICES_INSECURE:
                channel = grpc.insecure_channel(MITHRIL_SERVICES_URL)
            else:
                channel = grpc.secure_channel(
                    MITHRIL_SERVICES_URL,
                    ssl_channel_credentials(),
                )

            stub = licensing_pb2_grpc.LicensingServiceStub(channel)
            enclave_request = licensing_pb2.GetEnclaveRequest(api_key=api_key)

            response = stub.GetEnclave(enclave_request)
            host_ports = response.enclave_url.split(":")
            ports = host_ports[1].split("/")

            policy = Policy.from_str(MITHRIL_SERVICES_POLICY)

            addr = host_ports[0]
            attested_port = ports[0]
            untrusted_port = ports[1]

            self._jwt = response.jwt if len(response.jwt) > 0 else None

            if not response.private_cloud:
                log.info("Successfully connected to Mithril Security Public Cloud")
            else:
                log.info("Successfully connected to Mithril Security Private Cloud")

            log.debug(
                f"Selected enclave {response.enclave_url} & has jwt? {len(response.jwt) > 0}"
            )

        if policy is None and simulation is False:
            log.info("No policy specified. Using the built-in policy.")
            policy = Policy.from_str(MITHRIL_SERVICES_POLICY)

        uname = platform.uname()
        self.client_info = ClientInfo(
            uid=sha256((socket.gethostname() + "-" + getpass.getuser()).encode("utf-8"))
            .digest()
            .hex(),
            platform_name=uname.system,
            platform_arch=uname.machine,
            platform_version=uname.version,
            platform_release=uname.release,
            user_agent="blindai_python",
            user_agent_version=app_version,
        )

        self._connect_enclave(
            addr,
            server_name,
            policy,
            certificate,
            simulation,
            untrusted_port,
            attested_port,
            use_mithril_services=use_cloud,
        )

    def _connect_enclave(
        self,
        addr: str,
        server_name,
        policy,
        certificate,
        simulation,
        untrusted_port,
        attested_port,
        use_mithril_services: bool = False,
    ):
        self.simulation_mode = simulation

        addr = strip_https(addr)

        untrusted_client_to_enclave = addr + ":" + str(untrusted_port)
        attested_client_to_enclave = addr + ":" + str(attested_port)

        if not self.simulation_mode:
            self.policy = (
                policy if isinstance(policy, Policy) else Policy.from_file(policy)
            )

        if simulation:
            log.warning("Untrusted server certificate check bypassed")

        if (
            simulation
            or use_mithril_services
            or (simulation is False and certificate is None)
        ):
            if (
                simulation is False
                and certificate is None
                and use_mithril_services is False
            ):
                log.warning(
                    "No certificate specified. The certificate verification is disabled."
                )
            try:
                socket.setdefaulttimeout(CONNECTION_TIMEOUT)

                untrusted_server_cert = ssl.get_server_certificate(
                    (addr, untrusted_port)
                )
                untrusted_server_creds = ssl_channel_credentials(
                    root_certificates=bytes(untrusted_server_cert, encoding="utf8")
                )

            except RpcError as rpc_error:
                if rpc_error.code() == grpc.StatusCode.FAILED_PRECONDITION:
                    raise HardwareModeUnsupportedError(
                        "The server you are trying to reach is in Simulation mode. It cannot attest of its identity."
                    )
                else:
                    raise ConnectionError(check_rpc_exception(rpc_error))

            except socket.error as socket_error:
                raise ConnectionError(check_socket_exception(socket_error))

        else:
            with open(certificate, "rb") as f:
                untrusted_server_creds = ssl_channel_credentials(
                    root_certificates=f.read()
                )

        connection_options = (
            ("grpc.ssl_target_name_override", server_name),
            ("grpc.max_receive_message_length", 2**23),
        )

        try:
            channel = secure_channel(
                untrusted_client_to_enclave,
                untrusted_server_creds,
                options=connection_options,
            )
            stub = AttestationStub(channel)

            response = stub.GetServerInfo(
                server_info_request(), metadata=self._grpc_call_metadata()
            )
            self.server_version = response.version
            if not supported_server_version(response.version):
                raise VersionError(
                    f"Incompatible client/server versions. Please use the correct client for your server. Expected server version: {get_supported_server_version()}, Current server version: {self.server_version}"
                )

            if self.simulation_mode:
                log.warning(
                    "Attestation process is bypassed: running without requesting and checking attestation."
                )
                response = stub.GetCertificate(
                    certificate_request(), metadata=self._grpc_call_metadata()
                )
                server_cert = encode_certificate(response.enclave_tls_certificate)

            else:
                log.debug("Verifying server certificate...")
                self.attestation = stub.GetSgxQuoteWithCollateral(
                    quote_request(), metadata=self._grpc_call_metadata()
                )
                log.debug("Certificate verification passed")
                log.debug("Veryfing enclave identity and configuration...")
                claims = verify_dcap_attestation(
                    self.attestation.quote,
                    self.attestation.collateral,
                    self.attestation.enclave_held_data,
                )

                verify_claims(claims, self.policy)
                server_cert = claims.get_server_cert()

                log.info("MREnclave " + claims.sgx_mrenclave)
                log.info("Enclave identity and configuration verification passed")

            channel.close()
            self.enclave_signing_key = get_enclave_signing_key(server_cert)
            server_creds = ssl_channel_credentials(root_certificates=server_cert)
            channel = secure_channel(
                attested_client_to_enclave, server_creds, options=connection_options
            )

            self._stub = ExchangeStub(channel)
            self._channel = channel
            log.info("Successfuly connected to the trusted part of the enclave")

        except RpcError as rpc_error:
            channel.close()
            if rpc_error.code() == grpc.StatusCode.FAILED_PRECONDITION:
                raise HardwareModeUnsupportedError(
                    "The server you are trying to reach is in Simulation mode. It cannot attest of its identity."
                )
            else:
                raise ConnectionError(check_rpc_exception(rpc_error))

    @raise_exception_if_conn_closed
    def upload_model(
        self,
        model: str,
        input_specs: Optional[List[Tuple[List[int], ModelDatumType]]] = None,
        output_specs: Optional[List[ModelDatumType]] = None,
        shape: Tuple = None,
        dtype: ModelDatumType = None,
        dtype_out: ModelDatumType = None,
        sign: bool = False,
        model_id: Optional[str] = None,
        save_model: bool = True,
    ) -> UploadModelResponse:
        """Upload an inference model to the server.
        The provided model needs to be in the Onnx format.

        ***Security & confidentiality warnings:***
        *`model`: The model sent on a Onnx format is encrypted in transit via TLS (as all connections). It may be subject to inference Attacks if an adversary is able to query the trained model repeatedly to determine whether or not a particular example is part of the trained dataset model.
        `sign` : by enabling sign, DCAP attestation is verified by the SGX attestation model. This attestation model relies on Elliptic Curve Digital Signature algorithm (ECDSA).*


        Args:
            model (str): Path to Onnx model file.
            input_specs (List[Tuple[List[int], ModelDatumType]], optional): The list of input fact and datum types for each input grouped together in lists, describing the different inputs of the model. Can be left to None most of the time, as the server will retreive that information directly from the model, if available.
            output_specs (List[ModelDatumType], optional): The list of datum types describing the different output types of the model. Can be left to None most of the time, as the server will retreive that information directly from the model, if available.
            shape (Tuple, optional): The shape of the model input. Ignored if you are using models with multiple inputs. Can be left to None most of the time, as the server will retreive that information directly from the model, if available.
            datum_type (ModelDatumType, optional): The type of the model input data (f32 by default). Ignored if you are using models with multiple inputs. Can be left to None most of the time, as the server will retreive that information directly from the model, if available.
            dtype_out (ModelDatumType, optional): The type of the model output data (f32 by default). Ignored if you are using models with multiple outputs. Can be left to None most of the time, as the server will retreive that information directly from the model, if available.
            sign (bool, optional): Get signed responses from the server or not. Defaults to False.
            model_id (Optional[str], optional): Name of the model. By default, the server will assign a random UUID. You can call the model with the name you specify here.
            save_model (bool, optional): Whether or not the model will be saved to disk in the server. The model will be saved encrypted (sealed) so that only the server enclave can load it afterwards. Defaults to True.

        Raises:
            ConnectionError: Will be raised if the client is not connected.
            FileNotFoundError: Will be raised if the model file is not found.
            SignatureError: Will be raised if the response signature is invalid.
            ValueError: Will be raised if the connection is closed.

        Returns:
            UploadModelResponse: The response object.
        """
        response = None

        if model_id is None:
            model_name = os.path.basename(model)
        else:
            model_name = model_id
        try:
            with open(model, "rb") as f:
                data = f.read()

            (inputs, outputs) = _get_input_output_tensors(
                input_specs, output_specs, shape, dtype, dtype_out
            )
            log.debug(f"tensor specs: {inputs} {outputs}")
            log.debug("Uploading model...")
            data_size = len(data)
            response = self._stub.SendModel(
                (
                    PbSendModelRequest(
                        length=data_size,
                        data=chunk,
                        sign=sign,
                        model_id=model_id,
                        model_name=model_name,
                        client_info=self.client_info,
                        tensor_inputs=inputs,
                        tensor_outputs=outputs,
                        save_model=save_model,
                    )
                    for chunk in create_byte_chunk(data)
                ),
                metadata=self._grpc_call_metadata(),
            )

        except RpcError as rpc_error:
            raise ConnectionError(check_rpc_exception(rpc_error))

        # Response Verification
        payload = PbPayload.FromString(response.payload).send_model_payload
        ret = UploadModelResponse()
        ret.model_id = payload.model_id

        if sign:
            ret.payload = response.payload
            ret.signature = response.signature
            ret.attestation = self.attestation
            log.debug("Veryfing integrity of the data uploaded...")
            ret.validate(
                sha256(data).digest(),
                validate_quote=False,
                enclave_signing_key=self.enclave_signing_key,
                allow_simulation_mode=self.simulation_mode,
            )
            log.info("Integrity of the model uploaded verified.")
        else:
            log.warning("Integrity of the model uploaded NOT verified.")

        log.debug("Model uploaded successfully to the server")
        return ret

    @raise_exception_if_conn_closed
    def predict(
        self,
        model_id: str,
        tensors: Union[List[List[Any]], List[Any]],
        dtype: Optional[Union[List[ModelDatumType], ModelDatumType]] = None,
        shape: Optional[Union[List[List[int]], List[int]]] = None,
        sign: bool = False,
    ) -> PredictResponse:
        """
        Send data to the server to make a secure inference.

        The data provided must be in a list, as the tensor will be rebuilt inside the server.

        ***Security & confidentiality warnings:***
        *`model_id` : hash of the Onnx model uploaded. the given hash is return via gRPC through the proto files. It's a SHA-256 hash that is generated each time a model is uploaded.
        `tensors`: protected in transit and protected when running it on the secure enclave. In the case of a compromised OS, the data is isolated and confidential by SGX design.
        `sign`: by enabling sign, DCAP attestation is enabled to verify the SGX attestation model. This attestation model relies on Elliptic Curve Digital Signature algorithm (ECDSA).*

        Args:
            model_id (str): If set, will run a specific model.
            tensors (Union[List[Any], List[List[Any]]))): The input data. It must be an array of numpy, tensors or flat list of the same type datum_type specified in `upload_model`.
            dtype (Union[List[ModelDatumType], ModelDatumType], optional): The type of data of the data you want to upload. Only required if you are uploading flat lists, will be ignored if you are uploading numpy or tensors (this info will be extracted directly from the tensors/numpys).
            shape (Union[List[List[int]], List[int]], optional): The shape of the data you want to upload. Only required if you are uploading flat lists, will be ignored if you are uploading numpy or tensors (this info will be extracted directly from the tensors/numpys).
            sign (bool, optional): Get signed responses from the server or not. Defaults to False.
        Raises:
            ConnectionError: Will be raised if the client is not connected.
            SignatureError: Will be raised if the response signature is invalid
            ValueError: Will be raised if the connection is closed
        Returns:
            PredictResponse: The response object.
        """

        try:
            log.debug("Sending inference request...")
            response = self._stub.RunModel(
                (
                    # this will create an iterator of PbRunModelRequest, that will return
                    # chunks, one tensor input at a time
                    PbRunModelRequest(
                        model_id=model_id,
                        client_info=self.client_info,
                        input_tensors=[
                            PbTensorData(
                                info=PbTensorInfo(
                                    dims=tensor_shape,
                                    datum_type=tensor_dtype,
                                    index=i,  # only send the ith tensor for this call
                                ),
                                bytes_data=chunk,
                            )
                        ],
                        sign=sign,
                    )
                    for i, (tensor_iterable, tensor_dtype, tensor_shape) in enumerate(
                        translate_tensors(tensors, dtype, shape)
                    )
                    for chunk in serialize_tensor(tensor_iterable, tensor_dtype)
                ),
                metadata=self._grpc_call_metadata(),
            )

        except RpcError as rpc_error:
            raise ConnectionError(check_rpc_exception(rpc_error))

        log.debug("Inference done successfully by the server")
        return PredictResponse(
            tensors,
            dtype,
            shape,
            response,
            sign,
            self.attestation,
            self.enclave_signing_key,
            self.simulation_mode,
        )

    @raise_exception_if_conn_closed
    def delete_model(self, model_id: str) -> DeleteModelResponse:
        """Delete a model in the inference server.
        This may be used to free up some memory.
        If you did not specify that you wanted your model to be saved on the server, please note that the model will only be present in memory, and will disappear when the server close.

        ***Security & confidentiality warnings:***
            *model_id : If you are using this on the Mithril Security Cloud, you can only delete models that you uploaded. Otherwise, the deletion of a model does only relies on the `model_id`. It doesn't relies on a session token or anything, hence if the `model_id` is known, it's deletion is possible.*

        Args:
            model_id (str): The id of the model to remove.

        Raises:
            ConnectionError: Will be raised if the client is not connected or if an error happens during the connection.
            ValueError: Will be raised if the connection is closed.
        Returns:
            DeleteModelResponse: The response object.
        """
        try:
            self._stub.DeleteModel(
                PbDeleteModelRequest(model_id=model_id),
                metadata=self._grpc_call_metadata(),
            )

        except RpcError as rpc_error:
            raise ConnectionError(check_rpc_exception(rpc_error))

        return DeleteModelResponse()

    def close(self):
        """Close the connection between the client and the inference server. This method has no effect if the connection is already closed."""
        if not self.closed:
            self._channel.close()
            self.closed = True
            self._channel = None
            self._stub = None
            self.policy = None
            self.server_version = None

    def __enter__(self):
        """Return the Connection upon entering the runtime context."""
        return self

    def __exit__(self, *args):
        """Close the connection to BlindAI server and raise any exception triggered within the runtime context."""
        self.close()


@wraps(Connection.__init__, assigned=("__doc__", "__annotations__"))
def connect(*args, **kwargs):
    return Connection(*args, **kwargs)
