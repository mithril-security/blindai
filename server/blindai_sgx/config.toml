# [Unused Internal connection for Host -> Enclave communication
internal_host_to_enclave_url = "https://0.0.0.0:50054"

# Internal connection for Enclave -> Host communication
internal_enclave_to_host_url = "https://0.0.0.0:50053"

# Untrusted connection for Client -> Enclave communication
client_to_enclave_untrusted_url = "https://0.0.0.0:50052"

# Attested connection for Client -> Enclave communication
client_to_enclave_attested_url = "https://0.0.0.0:50051"

# Max model size in bytes (default value: 900mb)
max_model_size = 924288000

# Max input size in bytes (default value: 500mb)
max_input_size = 924288000

# Set the path for model saving
models_path = "./models"

# Set the maximal amount of model allowed in memory. Attempting to cross this limit will, if the model exists, unload an unused model in memory and unseal the requested model, if available.
max_model_store = 2

# Set the maximal amount of model allowed in memory for one user
max_loaded_model_per_user = 1

# Set the maximal amount of sealed model for one user
max_sealed_model_per_user = 3

# Allow user to send a model, or only use the models already uploaded.
allow_sendmodel = true

# Specify a number of days after which models are automatically cleaned up. if this is not specified, no cleanup will be done, if this is 0, models will be deleted at the net cleanup.
daily_model_cleanup = 7

# Models to load on startup
# [[load_models]]
# model_id = "gpt-neox-2.7b"
# path = "./gpt-neox-2.7b/gpt-neox-2.7b.onnx"
# input_facts = [{ datum_type = "I64", dims = [1, 7] }]
# no_optim = false
# 
# [[load_models]]
# model_id = "facenet"
# path = "./facenet.onnx"
