# [Unused Internal connection for Host -> Enclave communication
internal_host_to_enclave_url = "https://0.0.0.0:50054"

# Internal connection for Enclave -> Host communication
internal_enclave_to_host_url = "https://0.0.0.0:50053"

# Untrusted connection for Client -> Enclave communication
client_to_enclave_untrusted_url = "https://0.0.0.0:50052"

# Attested connection for Client -> Enclave communication
client_to_enclave_attested_url = "https://0.0.0.0:50051"

# Max model size in bytes (default value: 900mb)
max_model_size = 924288000

# Max input size in bytes (default value: 500mb)
max_input_size = 924288000

# Set the path for model saving
models_path = "./models"

# Set the maximal amount of model allowed in memory. Attempting to cross this limit will, if the model exists, unload an unused model in memory and unseal the requested model, if available.
max_model_store = 3

# Allow user to send a model, or only use the models already uploaded.
allow_sendmodel = true

# Allow model to be saved on disk and sealed
allow_model_sealing = false

# Models to load on startup
# [[load_models]]
# model_id = "gpt-neox-2.7b"
# path = "./gpt-neox-2.7b/gpt-neox-2.7b.onnx"
# input_facts = [{ datum_type = "I64", dims = [1, 7] }]
# no_optim = false
# 
# [[load_models]]
# model_id = "facenet"
# path = "./facenet.onnx"
