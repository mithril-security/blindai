{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "334419ad",
   "metadata": {},
   "source": [
    "# Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6625af10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers[onnx]\n",
      "  Using cached transformers-4.16.2-py3-none-any.whl (3.5 MB)\n",
      "Collecting sacremoses\n",
      "  Using cached sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
      "Requirement already satisfied: importlib-metadata in /root/blind/client/env/lib/python3.6/site-packages (from transformers[onnx]) (4.8.3)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Using cached huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
      "Collecting requests\n",
      "  Using cached requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /root/blind/client/env/lib/python3.6/site-packages (from transformers[onnx]) (21.3)\n",
      "Collecting numpy>=1.17\n",
      "  Using cached numpy-1.19.5-cp36-cp36m-manylinux2010_x86_64.whl (14.8 MB)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.4.1-py3-none-any.whl (9.9 kB)\n",
      "Collecting pyyaml>=5.1\n",
      "  Using cached PyYAML-6.0-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (603 kB)\n",
      "Collecting tokenizers!=0.11.3,>=0.10.1\n",
      "  Using cached tokenizers-0.11.4-cp36-cp36m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.8 MB)\n",
      "Collecting tqdm>=4.27\n",
      "  Using cached tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n",
      "Requirement already satisfied: dataclasses in /root/blind/client/env/lib/python3.6/site-packages (from transformers[onnx]) (0.8)\n",
      "Collecting regex!=2019.12.17\n",
      "  Using cached regex-2022.1.18-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (748 kB)\n",
      "Collecting onnxruntime>=1.4.0\n",
      "  Using cached onnxruntime-1.10.0-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "Collecting tf2onnx\n",
      "  Using cached tf2onnx-1.9.3-py3-none-any.whl (435 kB)\n",
      "Collecting onnxconverter-common\n",
      "  Using cached onnxconverter_common-1.9.0-py2.py3-none-any.whl (78 kB)\n",
      "Collecting onnxruntime-tools>=1.4.2\n",
      "  Using cached onnxruntime_tools-1.7.0-py3-none-any.whl (212 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/blind/client/env/lib/python3.6/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers[onnx]) (4.1.1)\n",
      "Requirement already satisfied: protobuf in /root/blind/client/env/lib/python3.6/site-packages (from onnxruntime>=1.4.0->transformers[onnx]) (3.19.4)\n",
      "Collecting flatbuffers\n",
      "  Using cached flatbuffers-2.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting onnx\n",
      "  Using cached onnx-1.10.2-cp36-cp36m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (12.7 MB)\n",
      "Collecting py-cpuinfo\n",
      "  Using cached py-cpuinfo-8.0.0.tar.gz (99 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting coloredlogs\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Collecting psutil\n",
      "  Using cached psutil-5.9.0-cp36-cp36m-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (279 kB)\n",
      "Collecting py3nvml\n",
      "  Using cached py3nvml-0.2.7-py3-none-any.whl (55 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /root/blind/client/env/lib/python3.6/site-packages (from packaging>=20.0->transformers[onnx]) (3.0.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /root/blind/client/env/lib/python3.6/site-packages (from importlib-metadata->transformers[onnx]) (3.6.0)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Using cached urllib3-1.26.8-py2.py3-none-any.whl (138 kB)\n",
      "Collecting charset-normalizer~=2.0.0\n",
      "  Downloading charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached certifi-2021.10.8-py2.py3-none-any.whl (149 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.3-py3-none-any.whl (61 kB)\n",
      "Requirement already satisfied: six in /root/blind/client/env/lib/python3.6/site-packages (from sacremoses->transformers[onnx]) (1.16.0)\n",
      "Collecting click\n",
      "  Using cached click-8.0.3-py3-none-any.whl (97 kB)\n",
      "Collecting joblib\n",
      "  Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "Collecting flatbuffers\n",
      "  Using cached flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting humanfriendly>=9.1\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Collecting xmltodict\n",
      "  Using cached xmltodict-0.12.0-py2.py3-none-any.whl (9.2 kB)\n",
      "Using legacy 'setup.py install' for py-cpuinfo, since package 'wheel' is not installed.\n",
      "Installing collected packages: urllib3, idna, charset-normalizer, certifi, xmltodict, tqdm, requests, regex, pyyaml, numpy, joblib, humanfriendly, filelock, click, tokenizers, sacremoses, py3nvml, py-cpuinfo, psutil, onnx, huggingface-hub, flatbuffers, coloredlogs, transformers, tf2onnx, onnxruntime-tools, onnxruntime, onnxconverter-common\n",
      "    Running setup.py install for py-cpuinfo ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed certifi-2021.10.8 charset-normalizer-2.0.12 click-8.0.3 coloredlogs-15.0.1 filelock-3.4.1 flatbuffers-1.12 huggingface-hub-0.4.0 humanfriendly-10.0 idna-3.3 joblib-1.1.0 numpy-1.19.5 onnx-1.10.2 onnxconverter-common-1.9.0 onnxruntime-1.10.0 onnxruntime-tools-1.7.0 psutil-5.9.0 py-cpuinfo-8.0.0 py3nvml-0.2.7 pyyaml-6.0 regex-2022.1.18 requests-2.27.1 sacremoses-0.0.47 tf2onnx-1.9.3 tokenizers-0.11.4 tqdm-4.62.3 transformers-4.16.2 urllib3-1.26.8 xmltodict-0.12.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers[onnx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bc9033d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: blindai in /root/blind/client/env/lib/python3.6/site-packages (0.1.1)\n",
      "Requirement already satisfied: toml in /root/blind/client/env/lib/python3.6/site-packages (from blindai) (0.10.2)\n",
      "Requirement already satisfied: bitstring in /root/blind/client/env/lib/python3.6/site-packages (from blindai) (3.1.9)\n",
      "Requirement already satisfied: cryptography>=35.0.0 in /root/blind/client/env/lib/python3.6/site-packages (from blindai) (36.0.1)\n",
      "Requirement already satisfied: cbor2 in /root/blind/client/env/lib/python3.6/site-packages (from blindai) (5.4.2.post1)\n",
      "Requirement already satisfied: grpcio in /root/blind/client/env/lib/python3.6/site-packages (from blindai) (1.43.0)\n",
      "Requirement already satisfied: grpcio-tools in /root/blind/client/env/lib/python3.6/site-packages (from blindai) (1.43.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /root/blind/client/env/lib/python3.6/site-packages (from cryptography>=35.0.0->blindai) (1.15.0)\n",
      "Requirement already satisfied: six>=1.5.2 in /root/blind/client/env/lib/python3.6/site-packages (from grpcio->blindai) (1.16.0)\n",
      "Requirement already satisfied: setuptools in /root/blind/client/env/lib/python3.6/site-packages (from grpcio-tools->blindai) (39.0.1)\n",
      "Requirement already satisfied: protobuf<4.0dev,>=3.5.0.post1 in /root/blind/client/env/lib/python3.6/site-packages (from grpcio-tools->blindai) (3.19.4)\n",
      "Requirement already satisfied: pycparser in /root/blind/client/env/lib/python3.6/site-packages (from cffi>=1.12->cryptography>=35.0.0->blindai) (2.21)\n"
     ]
    }
   ],
   "source": [
    "!pip install blindai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553770d3",
   "metadata": {},
   "source": [
    "# Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7cac160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fce19ed9ae846939a150c5470600b05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8594ba67c5c149929cd60fb3e5d84d23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/256M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification\n",
    "\n",
    "# Load the model\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82abe440",
   "metadata": {},
   "source": [
    "# Export the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf6fcc79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6df851ea3d684ef09aa1f878cf98c806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68319cbba52f400fa9e36536b3504ed4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11f27bdf80de49918f20e4299ad64ea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "import torch\n",
    "\n",
    "# Create dummy input for export\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "sentence = \"I love AI and privacy!\"\n",
    "inputs = tokenizer(sentence, padding = \"max_length\", max_length = 8, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "# Export the model\n",
    "torch.onnx.export(\n",
    "\tmodel, inputs, \"./distilbert-base-uncased.onnx\",\n",
    "\texport_params=True, opset_version=11,\n",
    "\tinput_names = ['input'], output_names = ['output'],\n",
    "\tdynamic_axes={'input' : {0 : 'batch_size'},\n",
    "\t'output' : {0 : 'batch_size'}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd8d46a",
   "metadata": {},
   "source": [
    "# Upload model to inference server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38d443f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-02-23 16:10:27--  https://raw.githubusercontent.com/mithril-security/blindai/master/examples/distilbert/hardware/host_server.pem\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1155 (1,1K) [text/plain]\n",
      "Saving to: ‘host_server.pem.1’\n",
      "\n",
      "host_server.pem.1   100%[===================>]   1,13K  --.-KB/s    in 0s      \n",
      "\n",
      "2022-02-23 16:10:27 (48,5 MB/s) - ‘host_server.pem.1’ saved [1155/1155]\n",
      "\n",
      "--2022-02-23 16:10:27--  https://raw.githubusercontent.com/mithril-security/blindai/master/examples/distilbert/hardware/policy.toml\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 881 [text/plain]\n",
      "Saving to: ‘policy.toml.1’\n",
      "\n",
      "policy.toml.1       100%[===================>]     881  --.-KB/s    in 0s      \n",
      "\n",
      "2022-02-23 16:10:27 (37,8 MB/s) - ‘policy.toml.1’ saved [881/881]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/mithril-security/blindai/master/examples/distilbert/hardware/host_server.pem\n",
    "!wget https://raw.githubusercontent.com/mithril-security/blindai/master/examples/distilbert/hardware/policy.toml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "155ddc68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ok: true\n",
       "msg: \"OK\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from blindai.client import BlindAiClient, ModelDatumType\n",
    "\n",
    "# Launch client\n",
    "client = BlindAiClient()\n",
    "\n",
    "client.connect_server(addr=\"localhost\", policy=\"policy.toml\", certificate=\"host_server.pem\")\n",
    "\n",
    "client.upload_model(model=\"./distilbert-base-uncased.onnx\", shape=inputs.shape, dtype=ModelDatumType.I64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df5c14e",
   "metadata": {},
   "source": [
    "# Send data for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8b6f2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "# Prepare the inputs\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "sentence = \"I love AI and privacy!\"\n",
    "inputs = tokenizer(sentence, padding = \"max_length\", max_length = 8)[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac8b4612",
   "metadata": {},
   "outputs": [],
   "source": [
    "from blindai.client import BlindAiClient\n",
    "\n",
    "# Load the client\n",
    "client = BlindAiClient()\n",
    "client.connect_server(addr=\"localhost\", policy=\"policy.toml\", certificate=\"host_server.pem\")\n",
    "\n",
    "# Get prediction\n",
    "response = client.run_model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bf49edd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "output: 0.0005601687589660287\n",
       "output: 0.06354495882987976\n",
       "ok: true\n",
       "msg: \"OK\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e609cfb",
   "metadata": {},
   "source": [
    "Here we can compare the results against the original prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c06fc11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[0.0006, 0.0635]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.tensor(inputs).unsqueeze(0)).logits.detach()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
