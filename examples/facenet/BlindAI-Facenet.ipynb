{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "463e6024",
   "metadata": {},
   "source": [
    "# About this example\n",
    "\n",
    "This example shows how you can run a Facenet model to perform Facial Recognition with confidentiality guarantees. \n",
    "\n",
    "By using BlindAI, people can send data for the AI to analyze their biometric data without having to fear privacy leaks.\n",
    "\n",
    "Facenet is a state-of-the art ResNet model for Facial Recogntion. You can learn more about it on [Facenet repository](https://github.com/timesler/facenet-pytorch)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3962be97",
   "metadata": {},
   "source": [
    "# Installing dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbda973b",
   "metadata": {},
   "source": [
    "Install the dependencies this example needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78255e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers[onnx] torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2889651b",
   "metadata": {},
   "source": [
    "Install the Facenet-pytorch library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43829608",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install facenet-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd99a83",
   "metadata": {},
   "source": [
    "Install the latest version of BlindAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02339f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install blindai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efa298c",
   "metadata": {},
   "source": [
    "# Preparing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef1bc79",
   "metadata": {},
   "source": [
    "The first step here is to prepare the model to perform facial recognition. \n",
    "\n",
    "To make it simpler, we will do an example where we will hardcode the database of biometric templates in the neural network itself. This works if the database of people to identify is fixed. For more dynamic workload, BlindAI can be adapted to suit this use case but we will not cover it here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fd5b71",
   "metadata": {},
   "source": [
    "First we load the pretrained Facenet model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fdf9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from facenet_pytorch import InceptionResnetV1\n",
    "import torch\n",
    "\n",
    "resnet = InceptionResnetV1(pretrained='vggface2').eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48299a01",
   "metadata": {},
   "source": [
    "We then download the people that will serve as our biometric database. The goal here is to use a neural network to see if a new person to be identified belongs to one of the three people registered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3494e052",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/mithril-security/blindai/master/examples/facenet/woman_0.jpg\n",
    "!wget https://raw.githubusercontent.com/mithril-security/blindai/master/examples/facenet/woman_1.jpg\n",
    "!wget https://raw.githubusercontent.com/mithril-security/blindai/master/examples/facenet/woman_2.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0345f0",
   "metadata": {},
   "source": [
    "We can have a look at our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cc2fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "files = [f\"woman_{i}.jpg\" for i in range(3)]\n",
    "\n",
    "display(Image.open(files[0]), Image.open(files[1]), Image.open(files[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b7a98b",
   "metadata": {},
   "source": [
    "Here we will do the enrollment phase, i.e. extract a template from each person, and store it. Those templates will be used as references to compute a similarity score when someone new comes in to be identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2fd2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "embeddings = []\n",
    "\n",
    "for file in files:\n",
    "    # We open each file and preprocess it\n",
    "    im = Image.open(file)\n",
    "    im = torch.tensor(np.asarray(im)).permute(2,0,1).unsqueeze(0) / 128.0 - 1\n",
    "    \n",
    "    # We make the tensor go through the ResNet to extract a template\n",
    "    embedding = resnet(im)\n",
    "    embeddings.append(embedding.squeeze(0))\n",
    "    \n",
    "# We stack everything in a matrix\n",
    "embeddings = torch.stack(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9e6466",
   "metadata": {},
   "source": [
    "Because the scoring will be done through a dot product of a new candidate template with the registered templates, we can implement this scoring as a matrix multiplication between the registered tempalte and the new template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ef9ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Create the scoring layer with a matrix multiplication\n",
    "scoring_layer = nn.Linear(512, 3, bias=False)\n",
    "\n",
    "# Store the computed embeddings inside\n",
    "scoring_layer.weight.data = embeddings\n",
    "\n",
    "full_network = nn.Sequential(\n",
    "    resnet,\n",
    "    scoring_layer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad40ef9",
   "metadata": {},
   "source": [
    "Before sending our model to BlindAI, we will how it performs in practice.\n",
    "\n",
    "Let's download a test set, containing a different picture of the second woman we registered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a480e5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/mithril-security/blindai/master/examples/facenet/woman_test.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa24bbc5",
   "metadata": {},
   "source": [
    "We can see below that the two pictures are indeed from the same person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031831fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_im = Image.open(\"woman_test.jpg\")\n",
    "display(test_im, Image.open(\"woman_1.jpg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56f42b5",
   "metadata": {},
   "source": [
    "We can now apply our full network, which will extract a template from the test image, and compute a dot product between the new templates and the registered templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f55849",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_im = torch.tensor(np.asarray(test_im)).permute(2,0,1).unsqueeze(0) / 128.0 - 1\n",
    "\n",
    "scores = full_network(test_im)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ea6b8a",
   "metadata": {},
   "source": [
    "We can see that the scores reflect the truth: the dot product of the embeddings of the test image with the first and third women are low, while the score is high with the second woman. This makes sense, as the neural network was trained to provide a high score for pictures of the same person, and make the score low for different people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3833d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c012be8b",
   "metadata": {},
   "source": [
    "Now we can export the model to be fed to BlindAI to deploy it with privacy guarantees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988ecc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(full_network,               # model being run\n",
    "                  test_im,                         # model input (or a tuple for multiple inputs)\n",
    "                  \"facenet.onnx\",   # where to save the model (can be a file or file-like object)\n",
    "                  export_params=True,        # store the trained parameter weights inside the model file\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8919ccb5",
   "metadata": {},
   "source": [
    "# Deployment on BlindAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155ddc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import blindai\n",
    "\n",
    "# Launch client\n",
    "with blindai.connect() as client:\n",
    "    response = client.upload_model(\n",
    "        model=\"./facenet.onnx\"\n",
    "    )\n",
    "    model_id = response.model_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889d5725",
   "metadata": {},
   "source": [
    "This securely uploads the model to the Mithril Cloud.\n",
    "If you wish to run this example on premise, you should read the [Deploy on Hardware](https://blindai.mithrilsecurity.io/en/latest/getting-started/deploy-on-hardware/) documentation page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501791e6",
   "metadata": {},
   "source": [
    "# Sending data for confidential prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3896eb0",
   "metadata": {},
   "source": [
    "Now it's time to check it's working live!\n",
    "\n",
    "We will just prepare some input for the model inside the secure enclave of BlindAI to process it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56eb415",
   "metadata": {},
   "source": [
    "First we prepare our input data, the test image we used before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15799bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "test_im = Image.open(\"woman_test.jpg\")\n",
    "test_im = torch.tensor(np.asarray(test_im)).permute(2,0,1).unsqueeze(0) / 128.0 - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb73769b",
   "metadata": {},
   "source": [
    "Now we can send the biometric data to be processed confidentially!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b48baf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with blindai.connect() as client:\n",
    "  response = client.predict(model_id, test_im)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea276d7",
   "metadata": {},
   "source": [
    "As we can see below, the results are quite similar from the regular inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43c74dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.output[0].as_flat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee978593",
   "metadata": {},
   "source": [
    "Et voila! We have been able to apply a start of the art model for facial recognition, without ever having to show the data in clear to the people operating the service!\n",
    "\n",
    "If you have liked this example, do not hesitate to drop a star on our [GitHub](https://github.com/mithril-security/blindai) and chat with us on our [Discord](https://discord.gg/TxEHagpWd4)!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.14 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "136980a1836242649e61aeb5c8543f27": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_98335c68e9104b8b82353b9eef1753e3",
      "max": 111898327,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cd2511f821ee497aa74d5f9dbc5d66cc",
      "value": 111898327
     }
    },
    "14c26e56cd04482d81910af17c22dc60": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_88e7b0b874ed49a9808ca05ed7f7eb93",
       "IPY_MODEL_136980a1836242649e61aeb5c8543f27",
       "IPY_MODEL_360fc697195245808bdea73d970d8819"
      ],
      "layout": "IPY_MODEL_3c0559434d924d2da365c3cf8607bb23"
     }
    },
    "360fc697195245808bdea73d970d8819": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_efe38de1352d476da055c09598564ec9",
      "placeholder": "​",
      "style": "IPY_MODEL_f13043da56ed4bfea6e6a782afe748e6",
      "value": " 107M/107M [00:01&lt;00:00, 111MB/s]"
     }
    },
    "3c0559434d924d2da365c3cf8607bb23": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "88e7b0b874ed49a9808ca05ed7f7eb93": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eb9fc03e1e8141b9951fc36dc3024420",
      "placeholder": "​",
      "style": "IPY_MODEL_a039190e67264b25b104193ae3c93a77",
      "value": "100%"
     }
    },
    "98335c68e9104b8b82353b9eef1753e3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a039190e67264b25b104193ae3c93a77": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cd2511f821ee497aa74d5f9dbc5d66cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "eb9fc03e1e8141b9951fc36dc3024420": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "efe38de1352d476da055c09598564ec9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f13043da56ed4bfea6e6a782afe748e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
