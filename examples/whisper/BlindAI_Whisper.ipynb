{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About this example\n",
    "\n",
    "This example shows how you can run [OpenAI Whisper](https://github.com/openai/whisper) to perform speech-to-text with privacy regarding user data. \n",
    "\n",
    "By using [BlindAI](https://github.com/mithril-security/blindai), people can send data for the AI to analyze their data without having to fear privacy leaks.\n",
    "\n",
    "[Whisper](https://openai.com/blog/whisper/) is a Transformers model, developed by OpenAI for speech-to-text. You can learn more about it in the paper [Robust Speech Recognition via Large-Scale Weak Supervision](https://cdn.openai.com/papers/whisper.pdf).\n",
    "\n",
    "This tutorial involves three steps:\n",
    "- Prepare the Whisper model to have an ONNX file. This step is optional as we have a pre-exported model.\n",
    "- Upload the model inside BlindAI.\n",
    "- Query the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will install the packages needed to run this sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Whisper\n",
    "\n",
    "The commands below will install the Python packages needed to use Whisper models and evaluate the transcription results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install git+https://github.com/openai/whisper.git\n",
    "! pip install jiwer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install BlindAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the latest version of BlindAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install blindai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) Preparing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will use OpenAI Whisper. The goal of this step is to get a Whisper model for speech-to-text inside an ONNX file, as BlindAI can only serve ONNX models.\n",
    "\n",
    "Because our model only outputs information about the next most likely token to be predicted, and not a whole sentence, we need to create a kind of meta model that will make use of the model to generate a sequence of tokens.\n",
    "\n",
    "This is why we will create a `MetaModel` that will leverage a `Whisper` model to output a sequence of tokens. Then we will export it.\n",
    "\n",
    "We detail the process below but you can go directly to the [Deployment on BlindAI](#b---deployment-on-blindai) section. A pre-exported model will be downloaded, so no need to generate yourself the ONNX file for Whisper speech-to-text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to get the model and tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    import tensorflow  # required in Colab to avoid protobuf compatibility issues\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import whisper\n",
    "import torchaudio\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LibriSpeech(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    A simple class to wrap LibriSpeech and trim/pad the audio to 30 seconds.\n",
    "    It will drop the last few seconds of a very small portion of the utterances.\n",
    "    \"\"\"\n",
    "    def __init__(self, split=\"test-clean\", device=DEVICE):\n",
    "        self.dataset = torchaudio.datasets.LIBRISPEECH(\n",
    "            root=os.path.expanduser(\"~/.cache\"),\n",
    "            url=split,\n",
    "            download=True,\n",
    "        )\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        audio, sample_rate, text, _, _, _ = self.dataset[item]\n",
    "        assert sample_rate == 16000\n",
    "        audio = whisper.pad_or_trim(audio.flatten()).to(self.device)\n",
    "        mel = whisper.log_mel_spectrogram(audio)\n",
    "        \n",
    "        return (mel, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = LibriSpeech(\"test-clean\")\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = whisper.load_model(\"tiny.en\")\n",
    "print(\n",
    "    f\"Model is {'multilingual' if model.is_multilingual else 'English-only'} \"\n",
    "    f\"and has {sum(np.prod(p.shape) for p in model.parameters()):,} parameters.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a PyTorch class, the `MetaModel` mentioned earlier, that would contain all the logic needed to do speech-to-text with token generations.\n",
    "\n",
    "The code is inspired from the `DecodingTask` class from [Whisper](https://github.com/openai/whisper/blob/main/whisper/decoding.py). \n",
    "\n",
    "We do this because BlindAI only supports ONNX models. So we need to package our model inference logic inside an ONNX format before sending it to a secure enclave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Tuple, Iterable, Optional, Sequence, Union, TYPE_CHECKING\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from whisper.audio import CHUNK_LENGTH\n",
    "from whisper.tokenizer import Tokenizer, get_tokenizer\n",
    "from whisper.utils import compression_ratio\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from whisper.model import Whisper\n",
    "\n",
    "from whisper.decoding import *\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class MetaModel(nn.Module):\n",
    "    inference: Inference\n",
    "    sequence_ranker: SequenceRanker\n",
    "    decoder: TokenDecoder\n",
    "    logit_filters: List[LogitFilter]\n",
    "\n",
    "    def __init__(self, model: \"Whisper\", options: DecodingOptions):\n",
    "        super(NNDecodingTask, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "        language = options.language or \"en\"\n",
    "        tokenizer = get_tokenizer(model.is_multilingual, language=language, task=options.task)\n",
    "        self.tokenizer: Tokenizer = tokenizer\n",
    "        self.options: DecodingOptions = self._verify_options(options)\n",
    "\n",
    "        self.n_group: int = options.beam_size or options.best_of or 1\n",
    "        self.n_ctx: int = model.dims.n_text_ctx\n",
    "        self.sample_len: int = options.sample_len or model.dims.n_text_ctx // 2\n",
    "\n",
    "        self.sot_sequence: Tuple[int] = tokenizer.sot_sequence\n",
    "        if self.options.without_timestamps:\n",
    "            self.sot_sequence = tokenizer.sot_sequence_including_notimestamps\n",
    "\n",
    "        self.initial_tokens: Tuple[int] = self._get_initial_tokens()\n",
    "        self.sample_begin: int = len(self.initial_tokens)\n",
    "        self.sot_index: int = self.initial_tokens.index(tokenizer.sot)\n",
    "\n",
    "        # inference: implements the forward pass through the decoder, including kv caching\n",
    "        self.inference = PyTorchInference(model, len(self.initial_tokens))\n",
    "\n",
    "        # sequence ranker: implements how to rank a group of sampled sequences\n",
    "        self.sequence_ranker = MaximumLikelihoodRanker(options.length_penalty)\n",
    "\n",
    "        # decoder: implements how to select the next tokens, given the autoregressive distribution\n",
    "        if options.beam_size is not None:\n",
    "            self.decoder = BeamSearchDecoder(\n",
    "                options.beam_size, tokenizer.eot, self.inference, options.patience\n",
    "            )\n",
    "        else:\n",
    "            self.decoder = GreedyDecoder(options.temperature, tokenizer.eot)\n",
    "\n",
    "        # logit filters: applies various rules to suppress or penalize certain tokens\n",
    "        self.logit_filters = []\n",
    "        if self.options.suppress_blank:\n",
    "            self.logit_filters.append(SuppressBlank(self.tokenizer, self.sample_begin))\n",
    "        if self.options.suppress_tokens:\n",
    "            self.logit_filters.append(SuppressTokens(self._get_suppress_tokens()))\n",
    "        if not options.without_timestamps:\n",
    "            precision = CHUNK_LENGTH / model.dims.n_audio_ctx  # usually 0.02 seconds\n",
    "            max_initial_timestamp_index = None\n",
    "            if options.max_initial_timestamp:\n",
    "                max_initial_timestamp_index = round(self.options.max_initial_timestamp / precision)\n",
    "            self.logit_filters.append(\n",
    "                ApplyTimestampRules(tokenizer, self.sample_begin, max_initial_timestamp_index)\n",
    "            )\n",
    "\n",
    "    def _verify_options(self, options: DecodingOptions) -> DecodingOptions:\n",
    "        if options.beam_size is not None and options.best_of is not None:\n",
    "            raise ValueError(\"beam_size and best_of can't be given together\")\n",
    "        if options.temperature == 0:\n",
    "            if options.best_of is not None:\n",
    "                raise ValueError(\"best_of with greedy sampling (T=0) is not compatible\")\n",
    "        if options.patience is not None and options.beam_size is None:\n",
    "            raise ValueError(\"patience requires beam_size to be given\")\n",
    "        if options.length_penalty is not None and not (0 <= options.length_penalty <= 1):\n",
    "            raise ValueError(\"length_penalty (alpha) should be a value between 0 and 1\")\n",
    "\n",
    "        return options\n",
    "\n",
    "    def _get_initial_tokens(self) -> Tuple[int]:\n",
    "        tokens = list(self.sot_sequence)\n",
    "        prefix = self.options.prefix\n",
    "        prompt = self.options.prompt\n",
    "\n",
    "        if prefix:\n",
    "            prefix_tokens = (\n",
    "                self.tokenizer.encode(\" \" + prefix.strip()) if isinstance(prefix, str) else prefix\n",
    "            )\n",
    "            if self.sample_len is not None:\n",
    "                max_prefix_len = self.n_ctx // 2 - self.sample_len\n",
    "                prefix_tokens = prefix_tokens[-max_prefix_len:]\n",
    "            tokens = tokens + prefix_tokens\n",
    "\n",
    "        if prompt:\n",
    "            prompt_tokens = (\n",
    "                self.tokenizer.encode(\" \" + prompt.strip()) if isinstance(prompt, str) else prompt\n",
    "            )\n",
    "            tokens = [self.tokenizer.sot_prev] + prompt_tokens[-(self.n_ctx // 2 - 1) :] + tokens\n",
    "\n",
    "        return tuple(tokens)\n",
    "\n",
    "    def _get_suppress_tokens(self) -> Tuple[int]:\n",
    "        suppress_tokens = self.options.suppress_tokens\n",
    "\n",
    "        if isinstance(suppress_tokens, str):\n",
    "            suppress_tokens = [int(t) for t in suppress_tokens.split(\",\")]\n",
    "\n",
    "        if -1 in suppress_tokens:\n",
    "            suppress_tokens = [t for t in suppress_tokens if t >= 0]\n",
    "            suppress_tokens.extend(self.tokenizer.non_speech_tokens)\n",
    "        elif suppress_tokens is None or len(suppress_tokens) == 0:\n",
    "            suppress_tokens = []  # interpret empty string as an empty list\n",
    "        else:\n",
    "            assert isinstance(suppress_tokens, list), \"suppress_tokens must be a list\"\n",
    "\n",
    "        suppress_tokens.extend(\n",
    "            [self.tokenizer.sot, self.tokenizer.sot_prev, self.tokenizer.sot_lm]\n",
    "        )\n",
    "        if self.tokenizer.no_speech is not None:\n",
    "            # no-speech probability is collected separately\n",
    "            suppress_tokens.append(self.tokenizer.no_speech)\n",
    "\n",
    "        return tuple(sorted(set(suppress_tokens)))\n",
    "\n",
    "    def _get_audio_features(self, mel: Tensor):\n",
    "        if self.options.fp16:\n",
    "            mel = mel.half()\n",
    "\n",
    "        if mel.shape[-2:] == (self.model.dims.n_audio_ctx, self.model.dims.n_audio_state):\n",
    "            # encoded audio features are given; skip audio encoding\n",
    "            audio_features = mel\n",
    "        else:\n",
    "            audio_features = self.model.encoder(mel)\n",
    "\n",
    "        if audio_features.dtype != (torch.float16 if self.options.fp16 else torch.float32):\n",
    "            return TypeError(f\"audio_features has an incorrect dtype: {audio_features.dtype}\")\n",
    "\n",
    "        return audio_features\n",
    "\n",
    "    def _detect_language(self, audio_features: Tensor, tokens: Tensor):\n",
    "        languages = [self.options.language] * audio_features.shape[0]\n",
    "        lang_probs = None\n",
    "\n",
    "        if self.options.language is None or self.options.task == \"lang_id\":\n",
    "            lang_tokens, lang_probs = self.model.detect_language(audio_features, self.tokenizer)\n",
    "            languages = [max(probs, key=probs.get) for probs in lang_probs]\n",
    "            if self.options.language is None:\n",
    "                tokens[:, self.sot_index + 1] = lang_tokens  # write language tokens\n",
    "\n",
    "        return languages, lang_probs\n",
    "\n",
    "    def _main_loop(self, audio_features: Tensor, tokens: Tensor):\n",
    "        assert audio_features.shape[0] == tokens.shape[0]\n",
    "        n_batch = tokens.shape[0]\n",
    "        sum_logprobs: Tensor = torch.zeros(n_batch, device=audio_features.device)\n",
    "        no_speech_probs = [np.nan] * n_batch\n",
    "\n",
    "        try:\n",
    "            for i in range(self.sample_len):\n",
    "                logits = self.inference.logits(tokens, audio_features)\n",
    "\n",
    "                if i == 0 and self.tokenizer.no_speech is not None:  # save no_speech_probs\n",
    "                    probs_at_sot = logits[:, self.sot_index].float().softmax(dim=-1)\n",
    "                    no_speech_probs = probs_at_sot[:, self.tokenizer.no_speech].tolist()\n",
    "\n",
    "                # now we need to consider the logits at the last token only\n",
    "                logits = logits[:, -1]\n",
    "\n",
    "                # apply the logit filters, e.g. for suppressing or applying penalty to\n",
    "                for logit_filter in self.logit_filters:\n",
    "                    logit_filter.apply(logits, tokens)\n",
    "\n",
    "                # expand the tokens tensor with the selected next tokens\n",
    "                tokens, completed = self.decoder.update(tokens, logits, sum_logprobs)\n",
    "\n",
    "                if completed or tokens.shape[-1] > self.n_ctx:\n",
    "                    break\n",
    "        finally:\n",
    "            self.inference.cleanup_caching()\n",
    "\n",
    "        return tokens, sum_logprobs, no_speech_probs\n",
    "\n",
    "    def forward(self, mel: Tensor):\n",
    "        self.decoder.reset()\n",
    "        tokenizer: Tokenizer = self.tokenizer\n",
    "        n_audio: int = mel.shape[0]\n",
    "\n",
    "        audio_features: Tensor = self._get_audio_features(mel)  # encoder forward pass\n",
    "        tokens: Tensor = torch.tensor([self.initial_tokens]).repeat(n_audio, 1)\n",
    "\n",
    "        # detect language if requested, overwriting the language token\n",
    "        languages, language_probs = self._detect_language(audio_features, tokens)\n",
    "        if self.options.task == \"lang_id\":\n",
    "            return [\n",
    "                DecodingResult(audio_features=features, language=language, language_probs=probs)\n",
    "                for features, language, probs in zip(audio_features, languages, language_probs)\n",
    "            ]\n",
    "\n",
    "        # repeat the audio & text tensors by the group size, for beam search or best-of-n sampling\n",
    "        audio_features = audio_features.repeat_interleave(self.n_group, dim=0)\n",
    "        tokens = tokens.repeat_interleave(self.n_group, dim=0).to(audio_features.device)\n",
    "\n",
    "        # call the main sampling loop\n",
    "        tokens, sum_logprobs, no_speech_probs = self._main_loop(audio_features, tokens)\n",
    "\n",
    "        # reshape the tensors to have (n_audio, n_group) as the first two dimensions\n",
    "        audio_features = audio_features[:: self.n_group]\n",
    "        no_speech_probs = no_speech_probs[:: self.n_group]\n",
    "        assert audio_features.shape[0] == len(no_speech_probs) == n_audio\n",
    "\n",
    "        tokens = tokens.reshape(n_audio, self.n_group, -1)\n",
    "        sum_logprobs = sum_logprobs.reshape(n_audio, self.n_group)\n",
    "\n",
    "        return tokens, sum_logprobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a `MetaModel` available, we can export it. Because tracing is used behind the scenes for PyTorch export to ONNX, we need to provide an example of data used for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mels, text = next(iter(loader))\n",
    "mel = mels[0].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just need to export this Torch model to ONNX before feeding it to BlindAI.\n",
    "\n",
    "**Caution**: the code below might take a while, it took 20 minutes on a GCP n1-standard-4 VM.\n",
    "\n",
    "You can uncomment the cell to run it yourself, but it's easier to just pull the pre-exported model we provide you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_len = 20\n",
    "\n",
    "# options = whisper.DecodingOptions(language=\"en\", without_timestamps=True, \n",
    "#                                   fp16 = False, sample_len=sample_len)\n",
    "\n",
    "# metamodel = NNDecodingTask(model, options)\n",
    "\n",
    "# model_name = f\"whisper_tiny_en_{sample_len}_tokens.onnx\"\n",
    "# torch.onnx.export(metamodel, mel, model_name,\n",
    "#                   export_params=True, opset_version=12, \n",
    "#                   operator_export_type=torch.onnx.OperatorExportTypes.ONNX_ATEN_FALLBACK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment on BlindAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did not generate the ONNX model following the previous section, we have one available for download that you can pull using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --quiet --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1wqg1F0UkEdm3KB7n1BjfRLHnzKU2-G5S' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1wqg1F0UkEdm3KB7n1BjfRLHnzKU2-G5S\" -O whisper_tiny_en_20_tokens.onnx && rm -rf /tmp/cookies.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can upload the model to BlindAI Cloud. To upload of the model, make sure you have an API key.\n",
    "\n",
    "You can get one on the [Mithril Cloud](https://cloud.mithrilsecurity.io/).\n",
    "\n",
    "You might get an error if the name you want to use is already taken, as models are uniquely identified by their `model_id`. We will implement namespace soon to avoid that. Meanwhile, you will have to choose a unique ID. We provide an example below to upload your model with a unique name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import blindai\n",
    "import uuid\n",
    "\n",
    "api_key = \"YOUR_API_KEY\" # Enter your API key here\n",
    "model_id = \"whisper-\" + str(uuid.uuid4())\n",
    "\n",
    "# Upload the ONNX file along with specs and model name\n",
    "with blindai.Connection(api_key=api_key) as client:\n",
    "    response = client.upload_model(\"whisper_tiny_en_20_tokens.onnx\", model_id=model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your model should now be loaded inside a secure enclave managed by BlindAI Cloud! You will just need to send data now for it to be analyzed securely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sending data for confidential prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to check it's working live!\n",
    "\n",
    "We will just prepare some input for the model inside the secure enclave of BlindAI to process it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ONNX model we uploaded previously is just able to output raw tokens, so we need to postprocess it to display text.\n",
    "\n",
    "We will do it with a `PostProcessingTask` class inspired once again from the `DecodingTask` class from Whisper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whisper.decoding import *\n",
    "\n",
    "class PostProcessingTask:\n",
    "    sequence_ranker: SequenceRanker\n",
    "    decoder: TokenDecoder\n",
    "\n",
    "    def __init__(self, n_text_ctx, is_multilingual, options: DecodingOptions):\n",
    "\n",
    "        language = options.language or \"en\"\n",
    "        tokenizer = get_tokenizer(is_multilingual, language=language, task=options.task)\n",
    "        self.tokenizer: Tokenizer = tokenizer\n",
    "        self.options: DecodingOptions = self._verify_options(options)\n",
    "\n",
    "        self.n_group: int = options.beam_size or options.best_of or 1\n",
    "        self.n_ctx: int = n_text_ctx\n",
    "        self.sample_len: int = options.sample_len or n_text_ctx // 2\n",
    "\n",
    "        self.sot_sequence: Tuple[int] = tokenizer.sot_sequence\n",
    "        if self.options.without_timestamps:\n",
    "            self.sot_sequence = tokenizer.sot_sequence_including_notimestamps\n",
    "\n",
    "        self.initial_tokens: Tuple[int] = self._get_initial_tokens()\n",
    "        self.sample_begin: int = len(self.initial_tokens)\n",
    "        self.sot_index: int = self.initial_tokens.index(tokenizer.sot)\n",
    "\n",
    "        # sequence ranker: implements how to rank a group of sampled sequences\n",
    "        self.sequence_ranker = MaximumLikelihoodRanker(options.length_penalty)\n",
    "        \n",
    "        self.decoder = GreedyDecoder(options.temperature, tokenizer.eot)\n",
    "\n",
    "    def _verify_options(self, options: DecodingOptions) -> DecodingOptions:\n",
    "        if options.beam_size is not None and options.best_of is not None:\n",
    "            raise ValueError(\"beam_size and best_of can't be given together\")\n",
    "        if options.temperature == 0:\n",
    "            if options.best_of is not None:\n",
    "                raise ValueError(\"best_of with greedy sampling (T=0) is not compatible\")\n",
    "        if options.patience is not None and options.beam_size is None:\n",
    "            raise ValueError(\"patience requires beam_size to be given\")\n",
    "        if options.length_penalty is not None and not (0 <= options.length_penalty <= 1):\n",
    "            raise ValueError(\"length_penalty (alpha) should be a value between 0 and 1\")\n",
    "\n",
    "        return options\n",
    "\n",
    "    def _get_initial_tokens(self) -> Tuple[int]:\n",
    "        tokens = list(self.sot_sequence)\n",
    "        prefix = self.options.prefix\n",
    "        prompt = self.options.prompt\n",
    "\n",
    "        if prefix:\n",
    "            prefix_tokens = (\n",
    "                self.tokenizer.encode(\" \" + prefix.strip()) if isinstance(prefix, str) else prefix\n",
    "            )\n",
    "            if self.sample_len is not None:\n",
    "                max_prefix_len = self.n_ctx // 2 - self.sample_len\n",
    "                prefix_tokens = prefix_tokens[-max_prefix_len:]\n",
    "            tokens = tokens + prefix_tokens\n",
    "\n",
    "        if prompt:\n",
    "            prompt_tokens = (\n",
    "                self.tokenizer.encode(\" \" + prompt.strip()) if isinstance(prompt, str) else prompt\n",
    "            )\n",
    "            tokens = [self.tokenizer.sot_prev] + prompt_tokens[-(self.n_ctx // 2 - 1) :] + tokens\n",
    "\n",
    "        return tuple(tokens)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def run(self, tokens: Tensor, sum_logprobs) -> List[DecodingResult]:\n",
    "        tokenizer: Tokenizer = self.tokenizer\n",
    "        # get the final candidates for each group, and slice between the first sampled token and EOT\n",
    "        tokens, sum_logprobs = self.decoder.finalize(tokens, sum_logprobs)\n",
    "        tokens: List[List[Tensor]] = [\n",
    "            [t[self.sample_begin : (t == tokenizer.eot).nonzero()[0, 0]] for t in s] for s in tokens\n",
    "        ]\n",
    "\n",
    "        # select the top-ranked sample in each group\n",
    "        selected = self.sequence_ranker.rank(tokens, sum_logprobs)\n",
    "        tokens: List[List[int]] = [t[i].tolist() for i, t in zip(selected, tokens)]\n",
    "        texts: List[str] = [tokenizer.decode(t).strip() for t in tokens]\n",
    "\n",
    "        return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict without timestamps for short-form transcription\n",
    "sample_len = 20\n",
    "\n",
    "options = whisper.DecodingOptions(language=\"en\", without_timestamps=True, sample_len=sample_len)\n",
    "\n",
    "n_text_ctx = model.dims.n_text_ctx or 448\n",
    "is_multilingual = model.is_multilingual or False\n",
    "\n",
    "postprocess = PostProcessingTask(n_text_ctx, is_multilingual, options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import blindai\n",
    "\n",
    "with blindai.Connection(api_key=api_key) as client:\n",
    "  # Send data to the model\n",
    "  prediction = client.predict(model_id, mels[3].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction.inference_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, sum_logprobs = prediction.output\n",
    "tokens = tokens.as_torch().unsqueeze(0).unsqueeze(0)\n",
    "sum_logprobs = sum_logprobs.as_torch().unsqueeze(0)\n",
    "\n",
    "texts = postprocess.run(tokens, sum_logprobs)\n",
    "texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et voila! We have been able to apply a start of the art model for speech-to-text, without ever having to show the data in clear to the people operating the service!\n",
    "\n",
    "If you have liked this example, do not hesitate to drop a star on our [GitHub](https://github.com/mithril-security/blindai) and chat with us on our [Discord](https://discord.gg/TxEHagpWd4)!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": [
    {
     "file_id": "https://github.com/openai/whisper/blob/master/notebooks/LibriSpeech.ipynb",
     "timestamp": 1665036932006
    }
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0a17a67cdf24495c88a097f4c0e26c8b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3c76173f89534fe2a1246169ea929484": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4fdc71e3ba78493ead771ab847de1102": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "57fc9db32a914153ac85a4b61a81a06b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "88b0d88e41734e3f9bcc3de3c9cfbf9b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_be384c0c3d7944c4a7948e8f292e91f5",
      "placeholder": "​",
      "style": "IPY_MODEL_0a17a67cdf24495c88a097f4c0e26c8b",
      "value": " 331M/331M [00:12&lt;00:00, 29.4MB/s]"
     }
    },
    "a90990c599a449949ff3a7c87dbad05f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "be384c0c3d7944c4a7948e8f292e91f5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c3ce78f34db843998f7f4c702f022e74": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dcb66f64b5264b34a75fe0c04bad89c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a90990c599a449949ff3a7c87dbad05f",
      "placeholder": "​",
      "style": "IPY_MODEL_57fc9db32a914153ac85a4b61a81a06b",
      "value": "100%"
     }
    },
    "e1a281e6f31e4eacbb59cd9db3c9e552": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_dcb66f64b5264b34a75fe0c04bad89c5",
       "IPY_MODEL_fbf493746bab46379bd6b9468932e0d9",
       "IPY_MODEL_88b0d88e41734e3f9bcc3de3c9cfbf9b"
      ],
      "layout": "IPY_MODEL_c3ce78f34db843998f7f4c702f022e74"
     }
    },
    "fbf493746bab46379bd6b9468932e0d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4fdc71e3ba78493ead771ab847de1102",
      "max": 346663984,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3c76173f89534fe2a1246169ea929484",
      "value": 346663984
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
