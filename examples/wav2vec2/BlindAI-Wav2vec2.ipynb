{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "463e6024",
   "metadata": {},
   "source": [
    "# About this example\n",
    "\n",
    "This example shows how you can run a Wav2Vec2 model to perform Speech-To-Text with confidentiality guarantees. \n",
    "\n",
    "By using BlindAI, people can send data for the AI to analyze their conversations without having to fear privacy leaks.\n",
    "\n",
    "Wav2Vec2 is a state-of-the art Transformers model for speech. You can learn more about it on [FAIR blog's post](https://ai.facebook.com/blog/wav2vec-20-learning-the-structure-of-speech-from-raw-audio/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3962be97",
   "metadata": {},
   "source": [
    "# Installing dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbda973b",
   "metadata": {},
   "source": [
    "Install the dependencies this example needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78255e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers[onnx] torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093a9236",
   "metadata": {},
   "source": [
    "We will need `librosa` to load the \"hello world\" audio file. You might need to downgrade `numpy` to 1.21 to make it work. The following commands should do the trick to install `librosa`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb1c5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q --upgrade numpy==1.21\n",
    "!pip install -q librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406ea52f",
   "metadata": {},
   "source": [
    "In addition, you might need to install `ffmpeg` to have a backend to process the wav file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d2990a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt-get install -y ffmpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd99a83",
   "metadata": {},
   "source": [
    "Install the latest version of BlindAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02339f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install blindai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efa298c",
   "metadata": {},
   "source": [
    "# Preparing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d0fefd",
   "metadata": {},
   "source": [
    "Here we will use a large Wav2Vec2 model. First step is to get the model and tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9916dcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "import torch\n",
    "\n",
    "# load model and processor\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b057a31",
   "metadata": {},
   "source": [
    "We can download an hello world audio file to be used as example. Let's download it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1374d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/mithril-security/blindai/raw/master/examples/wav2vec2/hello_world.wav"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b032cd2f",
   "metadata": {},
   "source": [
    "We can hear it here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d460db55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "ipd.Audio('hello_world.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af05eb7",
   "metadata": {},
   "source": [
    "We can then see the Wav2vec2 model in action on the hello world file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbd3a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "\n",
    "audio, rate = librosa.load(\"hello_world.wav\", sr = 16000)\n",
    "\n",
    "# Tokenize sampled audio to input into model\n",
    "input_values = processor(audio, sampling_rate=rate, return_tensors=\"pt\", padding=\"longest\").input_values\n",
    "\n",
    "# Retrieve logits\n",
    "logits = model(input_values).logits\n",
    "\n",
    "# Take argmax and decode\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "transcription = processor.batch_decode(predicted_ids)\n",
    "print(transcription)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a6a94b",
   "metadata": {},
   "source": [
    "In order to facilitate the deployment, we will add the post processing directly to the full model. This way the client will not have to do the post processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc89a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Let's embed the post-processing phase with argmax inside our model\n",
    "class ArgmaxLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ArgmaxLayer, self).__init__()\n",
    "\n",
    "    def forward(self, outputs):\n",
    "        return torch.argmax(outputs.logits, dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c98de98",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_layer = ArgmaxLayer()\n",
    "\n",
    "# Finally we concatenate everything\n",
    "full_model = nn.Sequential(model, final_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195cf33c",
   "metadata": {},
   "source": [
    "We can check the results are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e24a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_ids = full_model(input_values)\n",
    "transcription = processor.batch_decode(predicted_ids)\n",
    "transcription"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31eccfed",
   "metadata": {},
   "source": [
    "Now we can export the model in ONNX format, so that we can feed later the ONNX to our BlindAI server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bd9841",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(\n",
    "    full_model,\n",
    "    input_values,\n",
    "    'wav2vec2_hello_world.onnx',\n",
    "    export_params=True,\n",
    "     opset_version = 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8919ccb5",
   "metadata": {},
   "source": [
    "# Deployment on BlindAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2366fb",
   "metadata": {},
   "source": [
    "Please make sure the **server is running**. To launch the server, refer to the [Launching the server](https://docs.mithrilsecurity.io/getting-started/quick-start/run-the-blindai-server) documentation page. \n",
    "\n",
    "If you have followed the steps and have the Docker image ready, this mean you simply have to run `docker run -it -p 50051:50051 -p 50052:50052 mithrilsecuritysas/blindai-server-sim:latest`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fc49a9",
   "metadata": {},
   "source": [
    "So the first thing we need to do is to connect securely to the BlindAI server instance. Here we will use simulation mode for ease of use. This means that we do not leverage the hardware security propertiers of secure enclaves, but we do not need to run the Docker image with a specific hardware.\n",
    "\n",
    "If you wish to run this example in hardware mode, you need to prepare the `host_server.pem` and `policy.toml` files. Learn more on the [Deploy on Hardware](https://docs.mithrilsecurity.io/getting-started/deploy-on-hardware) documentation page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155ddc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import blindai.client\n",
    "from blindai.client import ModelDatumType\n",
    "\n",
    "# Launch client in simulation mode\n",
    "client = blindai.client.connect(addr=\"localhost\", simulation=True)\n",
    "\n",
    "# Launch client in hardware mode\n",
    "# client = blindai.client.connect(addr=\"localhost\", policy=\"./policy.toml\", certificate=\"./host_server.pem\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889d5725",
   "metadata": {},
   "source": [
    "Then, upload the model inside the BlindAI server. This simply means uploading the ONNX file created before.\n",
    "\n",
    "When uploading the model, we have to precise the shape of the input and the data type. \n",
    "\n",
    "In this case, because we use a Wav2vec2 model, we will need to send floats for the audio file. By default BlindAI outputs floats, but in this case we need tokens so we have to precise that we expect integers as outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f605f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.upload_model(model=\"./wav2vec2_hello_world.onnx\", shape=input_values.shape, \n",
    "                    dtype=ModelDatumType.F32, dtype_out=ModelDatumType.I64)\n",
    "model_id = response.model_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501791e6",
   "metadata": {},
   "source": [
    "# Sending data for confidential prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3896eb0",
   "metadata": {},
   "source": [
    "Now it's time to check it's working live!\n",
    "\n",
    "We will just prepare some input for the model inside the secure enclave of BlindAI to process it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56eb415",
   "metadata": {},
   "source": [
    "First we prepare our input data, the hello world audio file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15799bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "import torch\n",
    "import librosa\n",
    "\n",
    "# load model and processor\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "audio, rate = librosa.load(\"hello_world.wav\", sr = 16000)\n",
    "\n",
    "# Tokenize sampled audio to input into model\n",
    "input_values = processor(audio, sampling_rate=rate, return_tensors=\"pt\", padding=\"longest\").input_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb73769b",
   "metadata": {},
   "source": [
    "Now we can send the audio data to be processed confidentially!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b48baf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.run_model(model_id, input_values.flatten().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d89dce4",
   "metadata": {},
   "source": [
    "We can reconstruct the output now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cf9d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode the output\n",
    "print(processor.batch_decode(torch.tensor(response.output).unsqueeze(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bfb924",
   "metadata": {},
   "source": [
    "Et voila! We have been able to apply a start of the art model of speech recognition, without ever having to show the data in clear to the people operating the service!\n",
    "\n",
    "If you have liked this example, do not hesitate to drop a star on our [GitHub](https://github.com/mithril-security/blindai) and chat with us on our [Discord](https://discord.gg/TxEHagpWd4)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7fb05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.delete_model(model_id)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a5388ad2301bd502d89111dcdf0be4de1267e42deee031a75c954f6a954bbfcf"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
